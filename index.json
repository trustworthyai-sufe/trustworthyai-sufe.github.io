[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m now a joint doctoral student of ETH Zurich and Paul Sherrer Institut, supervised by Prof. Joachim Buhmann, Prof. Antony Lomax and Dr. Ye Zhang. Previously, I obtained my Master\u0026rsquo;s and Bachlor\u0026rsquo;s degree from Peking University and Beijing University of Posts and Telecommunications, respectively. During my postgrade, I was fortunate to work with Prof. Hong Liu and Prof. Zhouchen Lin.\nNow my research interest lies in the intersection of image processing technique and radiation therapy. My ultimate goal of my research is to reducing the cost of proton therapy from the algorithm perspective, and make it affordable by average people. Before studying medical image processing, I mainly work on image and video segmentation. I also have experience on image restoration and data mining.\n","date":1572586500,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1689331508,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://xialipku.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m now a joint doctoral student of ETH Zurich and Paul Sherrer Institut, supervised by Prof. Joachim Buhmann, Prof. Antony Lomax and Dr. Ye Zhang. Previously, I obtained my Master\u0026rsquo;s and Bachlor\u0026rsquo;s degree from Peking University and Beijing University of Posts and Telecommunications, respectively. During my postgrade, I was fortunate to work with Prof. Hong Liu and Prof. Zhouchen Lin.\nNow my research interest lies in the intersection of image processing technique and radiation therapy.","tags":null,"title":"Xia Li","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1606935606,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://xialipku.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://xialipku.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://xialipku.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Lei Ke","Xia Li","Martin Danelljan","Yu-Wing Tai","Chi-Keung Tang","Fisher Yu"],"categories":[],"content":"","date":1631446206,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635457446,"objectID":"3bdc88cd7f9267996e556e5dc123eeeb","permalink":"https://xialipku.github.io/publication/pcan-neurips21/","publishdate":"2021-09-12T12:30:06+01:00","relpermalink":"/publication/pcan-neurips21/","section":"publication","summary":"We propose Prototypical Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal information for online multiple object tracking and segmentation. ","tags":[],"title":"Prototypical Cross-Attention Networks for Multiple Object Tracking and Segmentation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"8b77add049d89c13d95332f3125877eb","permalink":"https://xialipku.github.io/post/tip21/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/post/tip21/","section":"post","summary":"","tags":null,"title":"One papers was accepted by TIP!","type":"post"},{"authors":["Xiangtai Li","Xia Li","Ansheng You","Li Zhang","Guangliang Cheng","Kuiyuan Yang","Yunhai Tong","Zhouchen Lin"],"categories":[],"content":"","date":1627574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"4c583c7585ce7f2139d7ddeb572ee490","permalink":"https://xialipku.github.io/publication/sqreason-tip21/","publishdate":"2021-07-30T00:00:00+08:00","relpermalink":"/publication/sqreason-tip21/","section":"publication","summary":"Graph-based convolutional model such as non-local block has shown to be effective for strengthening the context modeling ability in convolutional neural networks (CNNs). However, its pixel-wise computational overhead is prohibitive which renders it unsuitable for high resolution imagery. In this paper, we explore the efficiency of context graph reasoning and propose a novel framework called Squeeze Reasoning. Instead of propagating information on the spatial map, we first learn to squeeze the input feature into a channel-wise global vector and perform reasoning within the single vector where the computation cost can be significantly reduced. Specifically, we build the node graph in the vector where each node represents an abstract semantic concept. The refined feature within the same semantic category results to be consistent, which is thus beneficial for downstream tasks. We show that our approach can be modularized as an end-to-end trained block and can be easily plugged into existing networks. Despite its simplicity and being lightweight, the proposed strategy allows us to establish the considerable results on different semantic segmentation datasets and shows significant improvements with respect to strong baselines on various other scene understanding tasks including object detection, instance segmentation and panoptic segmentation. Code is available at https://github.com/lxtGH/SFSegNets.","tags":[],"title":"Towards efficient scene understanding via squeeze reasoning","type":"publication"},{"authors":["Jiangmiao Pang","Linlu Qiu","Xia Li","Qi Li","Trevor Darrel","Fisher Yu"],"categories":[],"content":"","date":1613827806,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"4bc120b03ee08e11cee9ccee366d8df8","permalink":"https://xialipku.github.io/publication/qdtrack-cvpr21/","publishdate":"2021-02-20T21:30:06+08:00","relpermalink":"/publication/qdtrack-cvpr21/","section":"publication","summary":"We present Quasi-Dense Similarity Learning, which densely samples hundreds of region proposals on a pair of images for contrastive learning.","tags":[],"title":"Quasi-dense similarity learning for multiple object tracking","type":"publication"},{"authors":null,"categories":null,"content":"","date":1613779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"7b333649571163248e288137aad609f8","permalink":"https://xialipku.github.io/post/cvpr21/","publishdate":"2021-02-20T00:00:00Z","relpermalink":"/post/cvpr21/","section":"post","summary":"","tags":null,"title":"Two papers was accepted by CVPR 2021!","type":"post"},{"authors":["Xiangtai Li","Hao He","Xia Li","Duo Li","Guangliang Cheng","Jianping Shi","Lubin Weng","Yunhai Tong","Zhouchen Lin"],"categories":[],"content":"","date":1613145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"d7d431453371a71798cd623c2b1d4710","permalink":"https://xialipku.github.io/publication/pointflow-cvpr21/","publishdate":"2021-02-13T00:00:00+08:00","relpermalink":"/publication/pointflow-cvpr21/","section":"publication","summary":"Aerial Image Segmentation is a particular semantic segmentation problem and has several challenging characteristics that general semantic segmentation does not have. There are two critical issues: The one is an extremely foreground-background imbalanced distribution and the other is multiple small objects along with complex background. Such problems make the recent dense affinity context modeling perform poorly even compared with baselines due to over-introduced background context. To handle these problems, we propose a point-wise affinity propagation module based on the FPN framework, named PointFlow. Rather than dense affinity learning, a sparse affinity map is generated upon selected points between the adjacent features, which reduces the noise introduced by the background while keeping efficiency. In particular, we design a dual point matcher to select points from the salient area and object boundaries, respectively. The former samples salient points while the latter samples points from the object boundaries. Experimental results on three different aerial segmentation datasets suggest that the proposed method is more effective and efficient than state-of-the-art general semantic segmentation methods. Especially, our methods achieve the best speed and accuracy trade-off on three aerial benchmarks. Further experiments on three general semantic segmentation datasets prove the generality of our method. Both code and models will be available for further research.","tags":[],"title":"PointFlow: Flowing Semantics Through Points for Aerial Image Segmentation","type":"publication"},{"authors":["Hanrong Ye","Hong Liu","Fanyang Meng","Xia Li"],"categories":[],"content":"","date":1607772606,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607772606,"objectID":"a0debef53cb2ac8f81b60a01c66afa4b","permalink":"https://xialipku.github.io/publication/atloss-tip2020/","publishdate":"2020-12-12T12:30:06+01:00","relpermalink":"/publication/atloss-tip2020/","section":"publication","summary":"We propose a novel ranking loss function, named Bi-directional Exponential Angular Triplet Loss, to help learn an angularly separable common feature space by explicitly constraining the included angles between embedding vectors.","tags":[],"title":"Bi-directional Exponential Angular Triplet Loss for RGB-Infrared Person Re-Identification","type":"publication"},{"authors":["Zhengyang Geng","Menghao Guo","Hongxu Chen","Xia Li","Ke Wei","Zhouchen Lin"],"categories":[],"content":"","date":1601299806,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"83867c4d30385351dd310204a78a66c4","permalink":"https://xialipku.github.io/publication/md-iclr20/","publishdate":"2020-09-28T21:30:06+08:00","relpermalink":"/publication/md-iclr20/","section":"publication","summary":"Self-attention is not better than the matrix decomposition~(MD) model developed 20 years ago regarding the performance and computational cost for encoding the long-distance dependencies.","tags":[],"title":"Is Attention Better Than Matrix Decomposition?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1601251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630836110,"objectID":"f1b7a0dad1a2850099ea3b154e869dad","permalink":"https://xialipku.github.io/post/iclr20/","publishdate":"2020-09-28T00:00:00Z","relpermalink":"/post/iclr20/","section":"post","summary":"","tags":null,"title":"One paper was accepted by ICLR 2020!","type":"post"},{"authors":["Xiangtai Li","Xia Li","Li Zhang","Guangliang Cheng","Jianping Shi","Zhouchen Lin","Shaohua Tan","Yunhai Tong"],"categories":[],"content":"","date":1593705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607936958,"objectID":"8556697119b629564f840f95974009b8","permalink":"https://xialipku.github.io/publication/decouple-eccv20/","publishdate":"2020-07-03T00:00:00+08:00","relpermalink":"/publication/decouple-eccv20/","section":"publication","summary":"Existing semantic segmentation approaches either aim to improve the object's inner consistency by modeling the global context, or refine objects detail along their boundaries by multi-scale feature fusion. In this paper, a new paradigm for semantic segmentation is proposed. Our insight is that appealing performance of semantic segmentation requires explicitly modeling the object body and edge, which correspond to the high and low frequency of the image. To do so, we first warp the image feature by learning a flow field to make the object part more consistent. The resulting body feature and the residual edge feature are further optimized under decoupled supervision by explicitly sampling different parts (body or edge) pixels. We show that the proposed framework with various baselines or backbone networks leads to better object inner consistency and object boundaries. Extensive experiments on four major road scene semantic segmentation benchmarks including Cityscapes, Camvid, KIITI and BDD show that our proposed approach establishes new state-of-the-arts while retaining high efficiency in inference. In particular, we achieve 83.7 mIoU % on Cityscape with only fine-annotated data. Code and models will be made available for future research. ","tags":[],"title":"Improving Semantic Segmentation via Decoupled Body and Edge Supervision","type":"publication"},{"authors":null,"categories":null,"content":"","date":1593648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"e597c36945d6cc32bd59b7966fc5d064","permalink":"https://xialipku.github.io/post/eccv20/","publishdate":"2020-07-02T00:00:00Z","relpermalink":"/post/eccv20/","section":"post","summary":"","tags":null,"title":"One paper was accepted by ECCV 2020!","type":"post"},{"authors":null,"categories":null,"content":"","date":1582502400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"63974616511369b5a12787a511007554","permalink":"https://xialipku.github.io/post/cvpr20/","publishdate":"2020-02-24T00:00:00Z","relpermalink":"/post/cvpr20/","section":"post","summary":"","tags":null,"title":"One paper was accepted by CVPR 2020!","type":"post"},{"authors":["Xia Li","Yibo Yang","Qijie Zhao","Tiancheng Shen","Zhouchen Lin","Hong Liu"],"categories":[],"content":"","date":1582473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591194202,"objectID":"e33c925d712eb463539072b1942c99b2","permalink":"https://xialipku.github.io/publication/spygr-cvpr20/","publishdate":"2019-05-01T00:00:00+08:00","relpermalink":"/publication/spygr-cvpr20/","section":"publication","summary":"The convolution operation suffers from a limited receptive filed, while global modeling is fundamental to dense prediction tasks, such as semantic segmentation. In this paper, we apply graph convolution into the semantic segmentation task and propose an improved Laplacian. The graph reasoning is directly performed in the original feature space organized as a spatial pyramid. Different from existing methods, our Laplacian is data-dependent and we introduce an attention diagonal matrix to learn a better distance metric. It gets rid of projecting and re-projecting processes, which makes our proposed method a light-weight module that can be easily plugged into current computer vision architectures. More importantly, performing graph reasoning directly in the feature space retains spatial relationships and makes spatial pyramid possible to explore multiple long-range contextual patterns from different scales. Experiments on Cityscapes, COCO Stuff, PASCAL Context and PASCAL VOC demonstrate the effectiveness of our proposed methods on semantic segmentation. We achieve state-of-the-art performance with advantages in computational and memory overhead. ","tags":[],"title":"Spatial Pyramid Based Graph Reasoning for Semantic Segmentation","type":"publication"},{"authors":["Yibo Yang","Hongyang Li","Xia Li","Qijie Zhao","Jianlong Wu","Zhouchen Lin"],"categories":[],"content":"","date":1581038348,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607936958,"objectID":"a62341697b0c9d14f62160a1454c440a","permalink":"https://xialipku.github.io/publication/sognet-aaai20/","publishdate":"2019-11-11T09:19:08+08:00","relpermalink":"/publication/sognet-aaai20/","section":"publication","summary":"We leverage each objectâ€™s category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances.","tags":[],"title":"SOGNet: Scene Overlap Graph Network for Panoptic Segmentation","type":"publication"},{"authors":["Yibo Yang","Jianlong Wu","Hongyang Li","Xia Li","Tiancheng Shen","Zhouchen Lin"],"categories":[],"content":"","date":1581035638,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607936958,"objectID":"4f484e48d9ec7b76a1202dc66637b9ca","permalink":"https://xialipku.github.io/publication/tsc-aaai20/","publishdate":"2019-11-11T08:33:58+08:00","relpermalink":"/publication/tsc-aaai20/","section":"publication","summary":"We analyze the effects of time stepping on the Euler method and ResNets. We establish a stability condition for ResNets with step sizes and weight parameters, and point out the effects of step sizes on the stability and performance. Inspired by our analyses, we develop an adaptive time stepping controller that is dependent on the parameters of the current step, and aware of previous steps.","tags":[],"title":"Dynamic System Inspired Adaptive Time Stepping Controller for Residual Networks Families","type":"publication"},{"authors":null,"categories":null,"content":"This websites of ZERO Lab and HRI Lab are both developed by me. And Now, I am maintaining ZERO Lab\u0026rsquo;s one.\n","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"db633a7b09989043faf3c155eff19fa6","permalink":"https://xialipku.github.io/post/zerolab/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/post/zerolab/","section":"post","summary":"This websites of ZERO Lab and HRI Lab are both developed by me. And Now, I am maintaining ZERO Lab\u0026rsquo;s one.","tags":null,"title":"The website of ZERO Lab is now online!","type":"post"},{"authors":null,"categories":null,"content":"","date":1573430400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"3d57269d0baafbb31ceecc4863b4c292","permalink":"https://xialipku.github.io/post/aaai20/","publishdate":"2019-11-11T00:00:00Z","relpermalink":"/post/aaai20/","section":"post","summary":"","tags":null,"title":"Two papers were accepted by AAAI 2020!","type":"post"},{"authors":["Xia Li"],"categories":null,"content":"","date":1572586500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"62b6c245829cd36f2a5ee11b66829194","permalink":"https://xialipku.github.io/talk/iccv19-oral/","publishdate":"2019-11-11T09:57:25+08:00","relpermalink":"/talk/iccv19-oral/","section":"talk","summary":"Oral presentation for EMANet","tags":[],"title":"ICCV Oral","type":"talk"},{"authors":["Yibo Yang","Xia Li","Hongyang Li","Tiancheng Shen"],"categories":null,"content":"","date":1572142200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"54b8a839758c0eacffdfae2006a4d279","permalink":"https://xialipku.github.io/talk/coco2019/","publishdate":"2019-07-23T11:40:10+08:00","relpermalink":"/talk/coco2019/","section":"talk","summary":"Panoptic Segmentation Innovative Award","tags":[],"title":"COCO2019","type":"talk"},{"authors":null,"categories":null,"content":"","date":1572134400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"112ddb9ae9a21b2c54b65c0ab0040f57","permalink":"https://xialipku.github.io/post/coco19/","publishdate":"2019-10-27T00:00:00Z","relpermalink":"/post/coco19/","section":"post","summary":"","tags":null,"title":"Our PKU_ZERO team won the Innovative Award in COCO 2019!","type":"post"},{"authors":["Xia Li","Zhisheng Zhong","Jianlong Wu","Yibo Yang","Zhouchen Lin","Hong Liu"],"categories":[],"content":"","date":1563802206,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"a1144895418e018f763f6b1a24571e93","permalink":"https://xialipku.github.io/publication/emanet-iccv19/","publishdate":"2019-07-19T21:30:06+08:00","relpermalink":"/publication/emanet-iccv19/","section":"publication","summary":"We formulate the attention mechanism into an expectation-maximization manner and iteratively estimate a much more compact set of bases upon which the attention maps are computed.","tags":[],"title":"Expectation Maximization Attention Networks for Semantic Segmentation","type":"publication"},{"authors":["Tiancheng Shen*","Xia Li*","Zhisheng Zhong","Jianlong Wu","Zhouchen Lin"],"categories":[],"content":"","date":1560949358,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"fbb46c70915728db5af71818d8be7a0a","permalink":"https://xialipku.github.io/publication/r2net-miccai19/","publishdate":"2019-07-19T21:02:38+08:00","relpermalink":"/publication/r2net-miccai19/","section":"publication","summary":"We propose a novel neural network architecture to reduce streak artifacts generated in sparse-view 2D Cone Beam Computed To-mography (CBCT) image reconstruction.","tags":[],"title":"R^2 Net Recurrent and Recursive Network for Sparse View CT Artifacts Removal","type":"publication"},{"authors":["Hong Liu","Hanrong Ye","Xia Li","Wei Shi","Mengyuan Liu","Qianru Sun"],"categories":[],"content":"","date":1558272114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"8cfe449d34aeb82b93d418caa764b22d","permalink":"https://xialipku.github.io/publication/dsen-icip19/","publishdate":"2019-07-19T21:21:54+08:00","relpermalink":"/publication/dsen-icip19/","section":"publication","summary":"We propose Deep Symmetry Enhanced Network (DSEN) thatis able to explicitly extract the rotation equivariant featuresfrom rain images.","tags":[],"title":"Self Refining Deep Symmetry Enhanced Network for Rain Removal","type":"publication"},{"authors":["Xiangtai Li","Xia Li"],"categories":null,"content":"","date":1554620400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"6f7388cc54ee628a453f115cbd344af1","permalink":"https://xialipku.github.io/talk/segmentation/","publishdate":"2019-07-23T11:31:15+08:00","relpermalink":"/talk/segmentation/","section":"talk","summary":"Shares the recent researches about semantic segmentation","tags":[],"title":"Recent researches on semantic segmentation","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view \r Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://xialipku.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Xia Li","Chaopeng Zhang","Yang Hu"],"categories":null,"content":"","date":1548396000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"0bd92db218bfbfcbc2a4f7503cd2886b","permalink":"https://xialipku.github.io/talk/ivfashion/","publishdate":"2019-07-23T11:16:17+08:00","relpermalink":"/talk/ivfashion/","section":"talk","summary":"The presentation of our team for Google AI ML Camp","tags":[],"title":"\"I love Fashion\"","type":"talk"},{"authors":["Yibo Yang","Xia Li","Hongyang Li","Tiancheng Shen"],"categories":null,"content":"","date":1536464700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"6ca31ae792fea0da626134b553ea3561","permalink":"https://xialipku.github.io/talk/coco2018/","publishdate":"2019-07-23T11:40:10+08:00","relpermalink":"/talk/coco2018/","section":"talk","summary":"Panoptic Segmentation Runner-Up","tags":[],"title":"COCO2018","type":"talk"},{"authors":["Xia Li","Jianlong Wu","Zhouchen Lin","Hong Liu","Hongbin Zha"],"categories":[],"content":"","date":1532001299,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"0f0fc1699568249a109dc4f475f35b5e","permalink":"https://xialipku.github.io/publication/rescan-eccv18/","publishdate":"2018-07-19T19:54:59+08:00","relpermalink":"/publication/rescan-eccv18/","section":"publication","summary":"We propose a novel deep network architecture based on deep convolutional and recurrent neural networksfor single image deraining.","tags":[],"title":"Recurrent Squeeze-and-Excitation Net for Single Image Deraining","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://xialipku.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606935606,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://xialipku.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]